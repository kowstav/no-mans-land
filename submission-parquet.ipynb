{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86023,"databundleVersionId":9869096,"sourceType":"competition"},{"sourceId":205183965,"sourceType":"kernelVersion"},{"sourceId":139552,"sourceType":"modelInstanceVersion","modelInstanceId":118183,"modelId":127417},{"sourceId":180858,"sourceType":"modelInstanceVersion","modelInstanceId":154124,"modelId":176602},{"sourceId":181353,"sourceType":"modelInstanceVersion","modelInstanceId":154560,"modelId":172131}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":803.741372,"end_time":"2024-10-27T06:12:38.713054","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-27T05:59:14.971682","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"3ae3375ceefd42f9aeede66494e08172":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_712906b89a034aa981c78ec917695c16","IPY_MODEL_6e02449d72574cb59a7408135ddf0606","IPY_MODEL_aabe4ff8b2734c9c86a33deb7a3d0a52"],"layout":"IPY_MODEL_d865f8a6c163451da7719a6d6e720f96"}},"63273838b2cf407a9d8a4b3d3c0a6096":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64efbe72d2c24a98b72db29e311e7206":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e02449d72574cb59a7408135ddf0606":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_805d7686fa99450fa5f28d53a2e50938","max":11,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c76df798b0e0495abc6131f26e897eca","value":11}},"712906b89a034aa981c78ec917695c16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af9c9877f8c347788ca34cdd8170fc94","placeholder":"​","style":"IPY_MODEL_c9c855af5c4941d6bed6542ce711a012","value":""}},"805d7686fa99450fa5f28d53a2e50938":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aabe4ff8b2734c9c86a33deb7a3d0a52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63273838b2cf407a9d8a4b3d3c0a6096","placeholder":"​","style":"IPY_MODEL_64efbe72d2c24a98b72db29e311e7206","value":"Loading safetensors checkpoint shards: 100% Completed | 11/11 [04:33&lt;00:00, 26.87s/it]\n"}},"af9c9877f8c347788ca34cdd8170fc94":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c76df798b0e0495abc6131f26e897eca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9c855af5c4941d6bed6542ce711a012":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d865f8a6c163451da7719a6d6e720f96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#fixed seed to get similar score\nfrom transformers import set_seed\nset_seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport warnings\n\nimport pandas as pd\nimport polars as pl\n\nimport torch\nimport kaggle_evaluation.aimo_2_inference_server\n\npd.set_option('display.max_colwidth', None)\ncutoff_time = time.time() + (4 * 60 + 45) * 60","metadata":{"papermill":{"duration":15.076705,"end_time":"2024-10-27T05:59:33.712437","exception":false,"start_time":"2024-10-27T05:59:18.635732","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"from vllm import LLM, SamplingParams\n\nwarnings.simplefilter('ignore')\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef clean_memory(deep=False):\n    gc.collect()\n    if deep:\n        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\n\nllm_model_pth = '/kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1'\n\nllm = LLM(\n    llm_model_pth,\n    #dtype=\"half\",                -> Changed this\n    #max_num_seqs=128,            -> Changed this\n    max_model_len=32768,#4096*10,         \n    trust_remote_code=True,     \n    tensor_parallel_size=4,      \n    gpu_memory_utilization=0.96, \n)","metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = llm.get_tokenizer()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utlities","metadata":{}},{"cell_type":"code","source":"import re\n\n#prompts\nthoughts = [\n    \"\"\"Act as five distinct mathematical experts specializing in tricky math problems, algebra, combinatorics, geometry, and number theory. Each expert will:\n    Identify their specialization and write down their initial step, explaining their reasoning thoroughly.\n    Share their response with the group for review.\n    Evaluate the responses of their peers, assigning a score from 1 to 5:\n    1: Highly unlikely.\n    5: Highly likely.\n    Experts who receive consistent low scores indicating errors will exit the discussion.    \n    Proceed step-by-step, refining the solution collaboratively, until the problem is fully addressed.\n    At the conclusion of the process, consolidate the insights from all experts to provide either a consensus solution or your best-informed answer, formatted as \\\\boxed{}.\"\"\",\n    \"\"\"Use a chain of reasoning to solve the given math problem. Then, simulate a debate between two groups of five experts each:\n    Group A:\n    Composed of experts specializing in tricky math problems, algebra, combinatorics, and number theory.\n    Argues in support of the proposed solution, explaining why it is correct and addressing potential doubts.\n    Group B:\n    Composed of similarly specialized experts.\n    Critiques the proposed solution, aiming to disprove it and present an alternate solution.\n    The debate will proceed until one group successfully convinces the other of its stance. At the conclusion of the process present a consensus solution, and put the answer in \\\\boxed{}.\"\"\",\n    'Solve the following problem using concise and clear reasoning by placing the answer in \\\\boxed{}.',\n    'You are a helpful and reflective maths assistant, please reason step by step to put the answer in \\\\boxed{}.',\n    \"\"\"As an expert in tricky math problems, algebra, combinatorics, and number theory, solve the given question using a chain of thought approach. Follow these steps:\n    Reason through the problem step-by-step, ensuring logical consistency at each stage.\n    Challenge and doubt your solution at every step, identifying potential errors or alternative interpretations.\n    Solve the problem using five distinct methods, leveraging your expertise in:\n    Algebraic manipulation.\n    Combinatorial analysis.\n    Number theory principles.\n    Geometric or graphical visualization.\n    Generalization and special-case testing.\n    Compare the outcomes of all five methods, evaluate their validity, and select the most likely correct solution.\n    Present the final answer in the format: \\boxed{solution}\n    \n    \"\"\"]\n\n#create single prompt\ndef make_next_prompt(text,round_idx):\n    default_prompt = thoughts[(round_idx+1)%len(thoughts)]\n    default_python_code = f\"print('{default_prompt}')\"\n    return default_python_code\n\n#extract python code from response\ndef extract_python_code(text):\n    pattern = r'```python\\s*(.*?)\\s*```'\n    matches = re.findall(pattern, text, re.DOTALL)\n    if matches:\n        ans = \"\\n\\n\".join(matches)\n        #print(f'Extracted python code: {ans}')\n        return ans\n    return \"\"\n\n#extract all code segments\ndef extract_python_code_list(text):\n    pattern = r'```python\\s*(.*?)\\s*```'\n    ans=[]\n    matches = re.findall(pattern, text, re.DOTALL)\n    for m in matches:\n        ans.append(m)\n    return ans\n\n#process the code\ndef process_python_code(query):\n    query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n    current_rows = query.strip().split(\"\\n\")\n    new_rows = []\n    for row in current_rows:\n        new_rows.append(row)\n    ans = \"\\n\".join(new_rows)\n    print(f'Processed python code: {ans}')\n    return ans\n\nimport re\n\n#extract the answer from the boxes\ndef extract_boxed_texts(text):\n    pattern = r'oxed{(.*?)}'\n    matches = re.findall(pattern, text)\n    if not matches:\n        return []\n    ans = []\n    for content in matches:\n        if content.isdigit():\n            num = int(content)\n        else:\n            nums = re.findall(r'\\d+', content)\n            if not nums:\n                continue\n            num = int(nums[-1])\n        ans.append(num % 1000)\n    return ans\n    \n#extract the integer answer modulo 1000 from the boxes\ndef extract_boxed_text(text):\n    pattern = r'oxed{(.*?)}'\n    matches = re.findall(pattern, text)\n    if not matches:\n        return -1\n    content = matches[0]\n    if content.isdigit():\n        num = int(content)\n    else:\n        nums = re.findall(r'\\d+', content)\n        if not nums:\n            return -1\n        num = int(nums[-1])\n    return num % 1000\n\n#select the final answer based on the frequency (majoity voting)\nfrom collections import Counter\ndef select_answer(answers):\n    valid_answers = []\n    for answer in answers:\n        try:\n            if int(answer) == float(answer):\n                if 1 < int(answer) < 999 and int(answer) % 100 > 0:\n                    valid_answers.append(int(answer))\n        except:\n            pass\n    if not valid_answers:\n        return 49\n    _, answer = sorted([(v,k) for k,v in Counter(valid_answers).items()], reverse=True)[0]\n    return answer%1000","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport tempfile\nimport subprocess\n\n#Python REPL to execute code. taken and modified from NuminaMath Solution\n#NuminaMath solution can be found here : https://www.kaggle.com/code/lewtun/numina-1st-place-solution\n\nclass PythonREPL:\n    def __init__(self, timeout=8):\n        self.timeout = timeout\n\n    def __call__(self, query):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(query)\n            \n            try:\n                result = subprocess.run(\n                    [\"python3\", temp_file_path],\n                    capture_output=True,\n                    check=False,\n                    text=True,\n                    timeout=self.timeout,\n                )\n            except subprocess.TimeoutExpired:\n                return False, f\"Execution timed out after {self.timeout} seconds.\"\n\n            stdout = result.stdout.strip()\n            stderr = result.stderr.strip()\n\n            if result.returncode == 0:\n                return True, stdout\n            else:\n                # Process the error message to remove the temporary file path\n                # This makes the error message cleaner and more user-friendly\n                error_lines = stderr.split(\"\\n\")\n                cleaned_errors = []\n                for line in error_lines:\n                    if temp_file_path in line:\n                        # Remove the path from the error line\n                        line = line.replace(temp_file_path, \"<temporary_file>\")\n                    cleaned_errors.append(line)\n                cleaned_error_msg = \"\\n\".join(cleaned_errors)\n                # Include stdout in the error case\n                combined_output = f\"{stdout}\\n{cleaned_error_msg}\" if stdout else cleaned_error_msg\n                return False, combined_output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ... (Code Set A remains the same until this point)\n\n# Python REPL and code execution utilities\nclass PythonREPL:\n    def __init__(self, timeout=5):\n        self.timeout = timeout\n\n    @contextmanager\n    def time_limit(self, seconds):\n        def signal_handler(*_):\n            raise TimeoutError(f\"Timed out after {seconds} seconds.\")\n\n        signal.signal(signal.SIGALRM, signal_handler)\n        signal.alarm(seconds)\n        try:\n            yield\n        finally:\n            signal.alarm(0)\n\n    def __call__(self, query):\n        query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n        query = query.strip().split(\"\\n\")\n        if \"print(\" not in query[-1]:\n            if \"#\" in query[-1]:\n                query[-1] = query[-1].split(\"#\")[0]\n            query[-1] = \"print(\" + query[-1] + \")\"\n        query = \"\\n\".join(query)\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(query)\n            with self.time_limit(self.timeout):\n                result = subprocess.run(\n                    [\"python3\", temp_file_path],\n                    capture_output=True,\n                    check=False,\n                    text=True,\n                    timeout=self.timeout,\n                )\n                if result.returncode == 0:\n                    output = result.stdout\n                    return True, output.strip()\n                error_msg = result.stderr.strip()\n                msgs = error_msg.split(\"\\n\")\n                new_msgs = []\n                want_next = False\n                for m in msgs:\n                    if \"Traceback\" in m:\n                        new_msgs.append(m)\n                    elif m == msgs[-1]:\n                        new_msgs.append(m)\n                    elif temp_file_path in m:\n                        st = m.index('\"/') + 1 if '\"/' in m else 0\n                        ed = m.index(temp_file_path) + 1 if temp_file_path in m else None\n                        clr = m[st:ed] if not ed else m[st:]\n                        m = m.replace(clr, \"\")\n                        new_msgs.append(m)\n                        want_next = True\n                    elif want_next:\n                        new_msgs.append(m)\n                        want_next = False\n                error_msg = \"\\n\".join(new_msgs)\n                return False, error_msg.strip()\n\n\ndef execute_completion(executor, completion, return_status, last_code_block):\n    executions = re.findall(r\"```python(.*?)```\", completion, re.DOTALL)\n    if len(executions) == 0:\n        return completion, False if return_status else completion\n    if last_code_block:\n        executions = [executions[-1]]\n    outputs = []\n    successes = []\n    for code in executions:\n        success = False\n        for lib in (\"subprocess\", \"venv\"):\n            if lib in code:\n                output = f\"{lib} is not allowed\"\n                outputs.append(output)\n                successes.append(success)\n                continue\n        try:\n            success, output = executor(code)\n        except TimeoutError as e:\n            print(\"Code timed out\")\n            output = e\n        if not success and not return_status:\n            output = \"\"\n        outputs.append(output)\n        successes.append(success)\n    output = str(outputs[-1]).strip()\n    success = successes[-1]\n    if return_status:\n        return output, success\n    return output\n\n\ndef postprocess_completion(text, return_status, last_code_block):\n    executor = PythonREPL()\n    result = execute_completion(executor, text, return_status=return_status, last_code_block=last_code_block)\n    del executor\n    return result\n\n\n# Rest of Code Set A remains the same","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#sanity check\nlist_of_texts = [\n    tokenizer.apply_chat_template(\n        messages,\n        tokenize=True,\n        add_generation_prompt=True\n    )\n    for messages in [[{\"role\": \"user\", \"content\": \"hi\"}]]\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text Generation Functions","metadata":{}},{"cell_type":"code","source":"\"\"\"# ... (rest of the code remains the same)\n\n# SC-TIR control flow (from code set 1)\ndef process_code(sample, restart_on_fail, last_step, check_last_n_chars=100):\n    gen_text = sample[\"gen_texts\"]\n    num_python_blocks = len(re.findall(r\"```python(.*?)```\", gen_text, re.DOTALL))\n    region_to_check = gen_text[-check_last_n_chars:]\n    if num_python_blocks == 0:\n        if restart_on_fail:\n            print(\"no code has ever been generated, RESTARTING\")\n            sample[\"gen_texts\"] = sample[\"text\"]\n        else:\n            print(\"no code has ever been generated, STOP\")\n            sample[\"should_prune\"] = True\n            sample[\"has_code\"] = False\n        return sample\n    if not gen_text.endswith(\"```output\\n\") and (\"answer is\" in region_to_check or \"\\\\boxed\" in region_to_check):\n        num_output_blocks = len(re.findall(r\"```output(.*?)```\", gen_text, re.DOTALL))\n        if num_output_blocks == 0:\n            print(\"The model hallucinated the code answer\")\n            sample[\"should_prune\"] = True\n            return sample\n        if \"boxed\" in region_to_check:\n            try:\n                answer = normalize_answer(extract_boxed_answer(region_to_check))\n            except Exception:\n                answer = \"-1\"\n        else:\n            answer = normalize_answer(region_to_check)\n        sample[\"model_answers\"] = answer\n        return sample\n    if last_step:\n        return sample\n    if not gen_text.endswith(\"```output\\n\"):\n        print(\"warning: output block not found: \", gen_text[-40:])\n        if restart_on_fail:\n            sample[\"gen_texts\"] = sample[\"text\"]\n        else:\n            sample[\"should_prune\"] = True\n        return sample\n    code_result, _ = postprocess_completion(gen_text, return_status=True, last_code_block=True)\n    truncation_limit = 200\n    if len(code_result) > truncation_limit:\n        code_result = code_result[:truncation_limit] + \" ... (output truncated)\"\n    sample[\"gen_texts\"] = gen_text + f\"{code_result}\\n```\"\n    return sample\n\n# ... (rest of the code remains the same)\n\n# Text Generation Functions\n#define the sampling parameters\nsampling_params = SamplingParams(\n    temperature=1.0,              # Controls randomness in generation: higher values (e.g., 1.0) produce more diverse output.\n    min_p=0.01,                   # Minimum cumulative probability for nucleus sampling, filtering out unlikely tokens.\n    skip_special_tokens=True,     \n    # max_tokens=1800,            \n    max_tokens=32768,             # Sets a very high limit for token generation to handle longer outputs.\n    # stop=[\"```output\"],       \n)\n\n#generate prompts in batch\ndef batch_message_generate(list_of_messages) -> list[list[dict]]:\n    list_of_texts = [\n        tokenizer.apply_chat_template(\n            conversation=messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        for messages in list_of_messages\n    ]\n    \n    request_output = llm.generate(\n        prompts=list_of_texts,\n        sampling_params=sampling_params,\n    )\n    \n    for messages, single_request_output in zip(list_of_messages, request_output):\n        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n        print(messages[-1])\n\n    # Apply SC-TIR control flow\n    for i, messages in enumerate(list_of_messages):\n        sample = {\n            \"gen_texts\": messages[-1]['content'],\n            \"text\": list_of_texts[i],\n            \"prompt\": question\n        }\n        sample = process_code(sample, restart_on_fail=True, last_step=False)\n        list_of_messages[i][-1]['content'] = sample[\"gen_texts\"]\n\n    return list_of_messages\n\n# ... (rest of the code remains the same)\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Post-processing and solution extraction utilities\nimport re\n\ndef extract_boxed_answer(text):\n    def last_boxed_only_string(text):\n        idx = text.rfind(\"\\\\boxed\")\n        if idx < 0:\n            idx = text.rfind(\"\\\\fbox\")\n            if idx < 0:\n                return None\n        i = idx\n        right_brace_idx = None\n        num_left_braces_open = 0\n        while i < len(text):\n            if text[i] == \"{\":\n                num_left_braces_open += 1\n            if text[i] == \"}\":\n                num_left_braces_open -= 1\n                if num_left_braces_open == 0:\n                    right_brace_idx = i\n                    break\n            i += 1\n        if right_brace_idx is None:\n            return None\n        return text[idx : right_brace_idx + 1]\n\n    def remove_boxed(boxed):\n        left = \"\\\\boxed{\"\n        try:\n            assert boxed[: len(left)] == left\n            assert boxed[-1] == \"}\"\n            length = len(left)\n            return boxed[length:-1]\n        except Exception:\n            return None\n\n    boxed = last_boxed_only_string(text)\n    if boxed is None:\n        return None\n    answer = remove_boxed(boxed)\n    return answer\n\n\ndef normalize_answer(answer):\n    match = re.search(r\"(.*?)Problem:\", answer, flags=re.S)\n    if match:\n        answer = match.group(1)\n    subs = [(\"an \", \"\"), (\"a \", \"\"), (\".$\", \"$\"), (\"\\\\$\", \"\"), (r\"\\ \", \"\"), (\" \", \"\"), (\"mbox\", \"text\"), (\",\\\\text{and}\", \",\"), (\"\\\\text{and}\", \",\"), (\"\\\\text{m}\", \"\\\\text{}\"), (\"\\\\le\", \"<\")]\n    remove = [\"square\", \"ways\", \"integers\", \"dollars\", \"mph\", \"inches\", \"ft\", \"hours\", \"km\", \"units\", \"\\\\ldots\", \"sue\", \"points\", \"feet\", \"minutes\", \"digits\", \"cents\", \"degrees\", \"cm\", \"gm\", \"pounds\", \"meters\", \"meals\", \"edges\", \"students\", \"childrentickets\", \"multiples\", \"\\\\text{s}\", \"\\\\text{.}\", \"\\\\text{\\ns}\", \"\\\\text{}^2\", \"\\\\text{}^3\", \"\\\\text{\\n}\", \"\\\\text{}\", r\"\\mathrm{th}\", r\"^\\circ\", r\"^{\\circ}\", r\"\\;\", r\",\\!\", \"{,}\", '\"', \"\\\\dots\", \"\\n\", \"\\r\", \"\\f\", \"\\%\"]\n    sub_patterns = [r\"(\\\\text\\{)(.*?)(\\})\", r\"(\\\\textbf\\{)(.*?)(\\})\", r\"(\\\\overline\\{)(.*?)(\\})\", r\"(\\\\boxed\\{)(.*)(\\})\"]\n    split_patterns = [r\"finalansweris(.*)\", r\"answer?is:?(.*)\", r\"oxed\\{(.*?)\\}\", r\"\\$(.*?)\\$\"]\n    for before, after in subs:\n        answer = answer.replace(before, after)\n    for expr in remove:\n        answer = answer.replace(expr, \"\")\n    for pattern in sub_patterns:\n        answer = re.sub(pattern, \"\\\\2\", answer)\n    for pattern in split_patterns:\n        if len(re.findall(pattern, answer)) > 0:\n            answer = re.findall(pattern, answer)[-1]\n    answer = answer.strip()\n    if \"rac\" in answer and \"\\\\frac\" not in answer:\n        answer = answer.replace(\"rac\", \"\\\\frac\")\n    answer = re.sub(r\"(frac)([^{])(.)\", \"frac{\\\\2}{\\\\3}\", answer)\n    answer = re.sub(r\"(sqrt)([^{])\", \"sqrt{\\\\2}\", answer)\n    answer = answer.replace(\"$\", \"\")\n    if answer.replace(\",\", \"\").isdigit():\n        answer = answer.replace(\",\", \"\")\n    return answer\n\n# Rest of Code Set A remains the same","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"def process_code(sample, restart_on_fail, last_step, check_last_n_chars=100,\n                   normalize_answer=None, extract_boxed_answer=None, postprocess_completion=None):\n    gen_text = sample[\"gen_texts\"]\n    num_python_blocks = len(re.findall(r\"```python(.*?)```\", gen_text, re.DOTALL))\n    region_to_check = gen_text[-check_last_n_chars:]\n    if num_python_blocks == 0:\n        if restart_on_fail:\n            print(\"no code has ever been generated, RESTARTING\")\n            sample[\"gen_texts\"] = sample[\"prompt\"]\n        else:\n            print(\"no code has ever been generated, STOP\")\n            sample[\"should_prune\"] = True\n            sample[\"has_code\"] = False\n        return sample\n    if not gen_text.endswith(\"```output\\n\") and (\"answer is\" in region_to_check or \"\\\\boxed\" in region_to_check):\n        num_output_blocks = len(re.findall(r\"```output(.*?)```\", gen_text, re.DOTALL))\n        if num_output_blocks == 0:\n            print(\"The model hallucinated the code answer\")\n            sample[\"should_prune\"] = True\n            return sample\n        if \"boxed\" in region_to_check:\n            try:\n                answer = normalize_answer(extract_boxed_answer(region_to_check))\n            except Exception:\n                answer = \"-1\"\n        else:\n            answer = normalize_answer(region_to_check)\n        sample[\"model_answers\"] = answer\n        return sample\n    if last_step:\n        return sample\n    if not gen_text.endswith(\"```output\\n\"):\n        print(\"warning: output block not found: \", gen_text[-40:])\n        if restart_on_fail:\n            sample[\"gen_texts\"] = sample[\"prompt\"]\n        else:\n            sample[\"should_prune\"] = True\n        return sample\n    code_result, _ = postprocess_completion(gen_text, return_status=True, last_code_block=True)\n    truncation_limit = 200\n    if len(code_result) > truncation_limit:\n        code_result = code_result[:truncation_limit] + \" ... (output truncated)\"\n    sample[\"gen_texts\"] = gen_text + f\"{code_result}\\n```\"\n    return sample\n\n\ndef batch_message_generate(list_of_messages) -> list[list[dict]]:\n    list_of_texts = [\n        tokenizer.apply_chat_template(\n            conversation=messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        for messages in list_of_messages\n    ]\n    \n    request_output = llm.generate(\n        prompts=list_of_texts,\n        sampling_params=sampling_params,\n    )\n    \n    for messages, single_request_output in zip(list_of_messages, request_output):\n        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n        print(messages[-1])\n\n    # Apply SC-TIR control flow\n    for i, messages in enumerate(list_of_messages):\n        sample = {\n            \"gen_texts\": messages[-1]['content'],\n            \"prompt\": messages[0]['content'],  # Use the original prompt\n            \"text\": list_of_texts[i],\n            \"should_prune\": False,\n            \"has_code\": True,\n            \"model_answers\": []\n        }\n        # Define missing functions or import them\n        def normalize_answer(answer):\n            return answer\n        \n        def extract_boxed_answer(text):\n            pattern = r'oxed{(.*?)}'\n            matches = re.findall(pattern, text)\n            if matches:\n                return matches[0]\n            return \"\"\n        \n        def postprocess_completion(gen_text, return_status=False, last_code_block=False):\n            return gen_text, \"\"\n\n        sample = process_code(sample, restart_on_fail=True, last_step=False, \n                                 normalize_answer=normalize_answer, \n                                 extract_boxed_answer=extract_boxed_answer, \n                                 postprocess_completion=postprocess_completion)\n        list_of_messages[i][-1]['content'] = sample[\"gen_texts\"]\n\n    return list_of_messages\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"#define the sampling parameters\nsampling_params = SamplingParams(\n    temperature=1.0,              # Controls randomness in generation: higher values (e.g., 1.0) produce more diverse output.\n    min_p=0.01,                   # Minimum cumulative probability for nucleus sampling, filtering out unlikely tokens.\n    skip_special_tokens=True,     \n    # max_tokens=1800,            \n    max_tokens=32768,             # Sets a very high limit for token generation to handle longer outputs.\n    # stop=[\"```output\"],       \n)\n\n#generate prompts in batch\ndef batch_message_generate(list_of_messages) -> list[list[dict]]:\n    list_of_texts = [\n        tokenizer.apply_chat_template(\n            conversation=messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        for messages in list_of_messages\n    ]\n    \n    request_output = llm.generate(\n        prompts=list_of_texts,\n        sampling_params=sampling_params,\n    )\n    \n    for messages, single_request_output in zip(list_of_messages, request_output):\n        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n        print(messages[-1])\n\n    return list_of_messages\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#filter answers from the responses\n\ndef batch_message_filter(list_of_messages,list_of_idx) -> tuple[list[list[dict]], list[str]]:\n    global answer_contributions\n    extracted_answers = []\n    list_of_messages_to_keep = []\n    list_of_idx_to_keep = []\n    for idx,messages in zip(list_of_idx,list_of_messages):\n        answers = extract_boxed_texts(messages[-1]['content'])\n        if answers:\n            extracted_answers.extend(answers)\n            for answer in answers:\n                answer_contributions[answer].append(idx)\n        else:\n            list_of_messages_to_keep.append(messages)\n            list_of_idx_to_keep.append(idx)\n    return list_of_messages_to_keep, extracted_answers, list_of_idx_to_keep","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#execute the codes in the responses\ndef batch_message_execute(list_of_messages,round_idx) -> list[list[dict]]:\n    for messages in list_of_messages:\n        python_code = extract_python_code(messages[-1]['content'],round_idx)\n        python_code = process_python_code(python_code)\n        try:\n            success, output = PythonREPL()(python_code)\n        except Exception as e:\n            output = str(e)\n        messages.append({'role': 'user', 'content': output})\n        print(messages[-1])\n    return list_of_messages\n\n#execute the code and generate the answer from responses\ndef batch_message_execute_and_get_answer(list_of_messages,round_idx) -> tuple[list[list[dict]], list[int]]:\n    ans = []\n    for messages in list_of_messages:\n        python_code = extract_python_code(messages[-1]['content'])\n        python_code = process_python_code(python_code)\n        try:\n            success, output = PythonREPL()(python_code)\n            if success:\n                patten = r'(\\d+)'\n                matches = re.findall(patten, output)\n                if matches:\n                    for match in matches:\n                        ans.append(int(match)%1000)\n                        ans.append(int(match)%1000) #代码权重高于自然语言，所以添加两次 \n        except Exception as e:\n            output = str(e)\n        print(f'python code output: {output}')\n    return ans\n\n#execute code and generate answer for all elements in batch\ndef batch_message_list_execute_and_get_answer(list_of_messages,round_idx) -> tuple[list[list[dict]], list[int]]:\n    ans = []\n    for messages in list_of_messages:\n        python_code_list = extract_python_code_list(messages[-1]['content'])\n        for python_code in python_code_list:\n            python_code = process_python_code(python_code)\n            try:\n                success, output = PythonREPL()(python_code)\n                if success:\n                    patten = r'(\\d+)'\n                    matches = re.findall(patten, output)\n                    if matches:\n                        for match in matches:\n                            ans.append(int(match)%1000)\n                            ans.append(int(match)%1000) \n            except Exception as e:\n                output = str(e)\n            print(f'python code output: {output}')\n    return ans","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SC-TIR control flow\ndef process_code(sample, restart_on_fail, last_step, check_last_n_chars=100):\n    gen_text = sample[\"gen_texts\"]\n    num_python_blocks = len(re.findall(r\"```python(.*?)```\", gen_text, re.DOTALL))\n    region_to_check = gen_text[-check_last_n_chars:]\n    if num_python_blocks == 0:\n        if restart_on_fail:\n            print(\"no code has ever been generated, RESTARTING\")\n            sample[\"gen_texts\"] = sample[\"text\"]\n        else:\n            print(\"no code has ever been generated, STOP\")\n            sample[\"should_prune\"] = True\n            sample[\"has_code\"] = False\n        return sample\n    if not gen_text.endswith(\"```output\\n\") and (\"answer is\" in region_to_check or \"\\\\boxed\" in region_to_check):\n        num_output_blocks = len(re.findall(r\"```output(.*?)```\", gen_text, re.DOTALL))\n        if num_output_blocks == 0:\n            print(\"The model hallucinated the code answer\")\n            sample[\"should_prune\"] = True\n            return sample\n        if \"boxed\" in region_to_check:\n            try:\n                answer = normalize_answer(extract_boxed_answer(region_to_check))\n            except Exception:\n                answer = \"-1\"\n        else:\n            answer = normalize_answer(region_to_check)\n        sample[\"model_answers\"] = answer\n        return sample\n    if last_step:\n        return sample\n    if not gen_text.endswith(\"```output\\n\"):\n        print(\"warning: output block not found: \", gen_text[-40:])\n        if restart_on_fail:\n            sample[\"gen_texts\"] = sample[\"text\"]\n        else:\n            sample[\"should_prune\"] = True\n        return sample\n    code_result, _ = postprocess_completion(gen_text, return_status=True, last_code_block=True)\n    truncation_limit = 200\n    if len(code_result) > truncation_limit:\n        code_result = code_result[:truncation_limit] + \" ... (output truncated)\"\n    sample[\"gen_texts\"] = gen_text + f\"{code_result}\\n```\"\n    return sample\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport polars as pl\n\n#API for competition submission\nimport kaggle_evaluation.aimo_2_inference_server","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#correct answers for reference problems\ndef get_correct_answer(question):\n    if 'Three airline' in question: return 79\n    if 'Fred and George' in question: return 250\n    if 'Triangle $ABC$' in question: return 180\n    if 'Find the three' in question: return 143\n    if 'We call a' in question: return 3\n    if 'Let $ABC$ be' in question: return 751\n    if 'For a positive' in question: return 891\n    if 'For positive integers' in question: return 810\n    if 'The Fibonacci numbers' in question: return 201\n    if 'Alice writes all' in question: return 902\n    return 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def predict_for_question(question: str) -> int:\n    global g_score\n    global g_count\n    global prompt_score\n    global answer_contributions\n    question += \"\\nIf the final answer is a number larger than 1000, take modulo 1000. \"\n    # ... (rest of the function implementation)\n    sample = {\n        \"text\": problem[\"text\"],\n        \"gen_texts\": problem[\"text\"],\n        \"should_prune\": False,\n        \"model_answers\": \"-1\",\n        \"has_code\": True,\n    }\n    sample = process_code(sample, restart_on_fail=True, last_step=False)\n    # ... (rest of the function implementation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main(config):\n    print(f\"=== Running submission with config ===\\n\\n{config}\")\n    set_seed(42)\n    num_procs = os.cpu_count()\n    vllm = build_vllm(config)\n    sampling_params = SamplingParams(\n        temperature=config.temperature,\n        max_tokens=config.max_new_tokens,\n        stop=[\"```output\\n\"],\n        include_stop_str_in_output=True,\n    )\n    env, iter_test = get_kaggle_env(config)\n    final_answers = []\n    for test, submission in tqdm(iter_test, desc=\"Solving problems\"):\n        problem = apply_template({\"prompt\": test.problem.values[0]}, tokenizer=vllm.get_tokenizer(), prompt=\"{}\")\n        print(f\"=== INPUT FOR PROBLEM ID {test.id.values[0]} ===\\n{problem}\\n\")\n        samples = Dataset.from_list([\n            {\n                \"text\": problem[\"text\"],\n                \"gen_texts\": problem[\"text\"],\n                \"should_prune\": False,\n                \"model_answers\": \"-1\",\n                \"has_code\": True,\n            }\n            for _ in range(config.num_samples)\n        ])\n        completed = []\n        for step in range(config.num_generations):\n            samples = samples.map(\n                generate_batched,\n                batch_size=128,\n                batched=True,\n                fn_kwargs={\"vllm\": vllm, \"sampling_params\": sampling_params},\n                load_from_cache_file=False,\n            )\n            samples = samples.map(\n                process_code,\n                num_proc=num_procs,\n                load_from_cache_file=False,\n                fn_kwargs={\"restart_on_fail\": config.restart_on_fail, \"last_step\": step == (config.num_generations - 1)},\n            )\n            done = samples.filter(lambda x: x[\"should_prune\"] is True, load_from_cache_file=False)\n            if len(done):\n                completed.append(done)\n            samples = samples.filter(lambda x: x[\"should_prune\"] is False, load_from_cache_file=False)\n        completed.append(samples)\n        samples = concatenate_datasets(completed)\n        candidates = samples[\"model_answers\"]\n        print(f\"=== CANDIDATE ANSWERS ({len(candidates)}) ===\\n{candidates}\\n\")\n        filtered = filter_answers(candidates)\n        print(f\"=== FILTERED ANSWERS ({len(filtered)}) ===\\n{filtered}\\n\")\n        majority = get_majority_vote(filtered)\n        print(f\"=== MAJORITY ANSWER (mod 1000) ===\\n{majority}\\n\")\n        submission[\"answer\"] = majority\n        env.predict(submission)\n        test[\"model_answer\"] = majority\n        final_answers.append(test)\n    if not config.is_submission:\n        answers = env.df.merge(pd.concat(final_answers))\n        answers[\"correct\"] = answers[\"ground_truth\"].astype(int) == answers[\"model_answer\"].astype(int)\n        print(\"Accuracy\", answers[\"correct\"].astype(int).mean())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#predict function to solve single problem\n\nfrom collections import Counter, defaultdict\ng_score = 0\ng_count = 0\nprompt_score = Counter()\nanswer_contributions = defaultdict(list)\ndef predict_for_question(question: str) -> int:\n    global g_score\n    global g_count\n    global prompt_score\n    global answer_contributions\n    question += \"\\nIf the final answer is a number larger than 1000, take modulo 1000. \"\n    if time.time() > cutoff_time: \n        return 210\n    print(question)\n    sample = {\n        \"text\": problem[\"text\"],\n        \"gen_texts\": problem[\"text\"],\n        \"should_prune\": False,\n        \"model_answers\": \"-1\",\n        \"has_code\": True,\n    }\n    sample = process_code(sample, restart_on_fail=True, last_step=False)\n    list_of_messages = [\n        [\n            {\"role\": \"system\", \"content\": thoughts[k]},\n            {\"role\": \"user\", \"content\": question}\n        ] for k in range(5)\n    ]\n\n    all_extracted_answers = []\n    list_of_idx = list(range(len(list_of_messages)))\n    max_round = 1\n    for round_idx in range(max_round):\n        print(f\"round {round_idx+1}\")\n        list_of_messages = batch_message_generate(list_of_messages)\n        #extracted_python_answer = batch_message_execute_and_get_answer(list_of_messages,round_idx)\n        extracted_python_answer = batch_message_list_execute_and_get_answer(list_of_messages,round_idx)\n        list_of_messages, extracted_answers, list_of_idx  = batch_message_filter(list_of_messages, list_of_idx)\n        all_extracted_answers.extend(extracted_python_answer)\n        all_extracted_answers.extend(extracted_answers)\n        print(\"extracted boxed answers:\",extracted_answers)\n        print(\"extracted python answers:\",extracted_python_answer)\n        print(\"all extracted answers:\",all_extracted_answers)\n        if not list_of_messages:\n            break\n        #list_of_messages = batch_message_execute(list_of_messages,round_idx)\n    answer = select_answer(all_extracted_answers)\n    print(\"answer:\",answer)\n    correct_answer = get_correct_answer(question)\n    print(\"correct answer:\",correct_answer)\n    g_count += 1\n    if str(answer) == str(correct_answer):\n        g_score += 1\n\n    print(f\"score: {g_score}/{g_count}\")\n    print(\"\\n\\n\")\n    return answer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Predict Function provided by the hosts**","metadata":{}},{"cell_type":"code","source":"# Replace this function with your inference code.\n# The function should return a single integer between 0 and 999, inclusive.\n# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\ndef predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n    id_ = id_.item(0)\n    print(\"------\")\n    print(id_)\n    \n    question = question.item(0)\n    answer = predict_for_question(question)\n    print(question)\n    print(\"------\\n\\n\\n\")\n    return pl.DataFrame({'id': id_, 'answer': answer})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace this function with your inference code.\n# The function should return a single integer between 0 and 999, inclusive.\n# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\n#def predict(id_, question):\n#    answer = predict_for_question(question)\n#    return {'id': id_, 'answer': answer}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Load and save the reference set to use for validation**","metadata":{}},{"cell_type":"code","source":"pd.read_csv(\n    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n).drop('answer', axis=1).to_csv('reference.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df = pd.read_csv('/kaggle/working/reference.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#results = []\n#for i in range(len(df)):\n#    id_ = df['id'][i]\n#    question = df['problem'][i]\n#    result = predict(id_, question)\n#    results.append(result)\n#r_df = pd.DataFrame(results)\n#print(r_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            'reference.csv',\n        )\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}